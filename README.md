# Databricks_Challenge
This repository documents my learning and hands-on implementation as part of the **Databricks 14 Days Challenge**, focused on understanding **Lakehouse architecture**, **Apache Spark**, and **Delta Lake** using Databricks Community Edition.

Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in the cloud account, and manages and deploys cloud infrastructure.

---

## ğŸ“Œ Objective
- Understand why Databricks is used over Pandas and Hadoop for large-scale data processing
- Learn Lakehouse architecture fundamentals
- Learn Apache Spark fundamentals, Lazy Evaluation
- Magic Commands in Databricks
- Work with Databricks workspace, compute, and data organization
- Build scalable data pipelines using Spark and Delta Lake
- Follow Medallion Architecture (Bronze, Silver, Gold)

---

## ğŸ› ï¸ Tools & Technologies
- Databricks Community Edition
- Apache Spark (PySpark)
- Delta Lake
- Cloud Object Storage (via Databricks Volumes)
- Python & SQL

---

## ğŸ“‚ Dataset
**E-commerce Behavior Dataset**  
Source: Kaggle  
Contains user interaction events such as views, cart additions, and purchases.

---

## ğŸ§± Architecture Overview
This project follows **Lakehouse architecture**, where:
- Data is stored in cloud object storage (data lake)
- Delta Lake adds reliability and structure
- Data flows through Bronze â†’ Silver â†’ Gold layers

---

## ğŸ“… Day-wise Progress

### âœ… Day 0 â€“ Setup & Data Loading
- Created Databricks Community Edition account
- Configured Kaggle API credentials
- Created schema and volume in Databricks
- Downloaded and extracted dataset into Databricks Volume
- Loaded October and November 2019 data using Spark

### âœ… Day 1 â€“ Platform Setup & First Steps
- Understood why Databricks vs Pandas/Hadoop
- Learned Lakehouse architecture basics
- Explored Databricks workspace structure
- Studied real-world use cases (Netflix, Shell, Comcast)
- Created first PySpark notebook
- Implemented reusable data loading function
- Ran basic Spark DataFrame operations

### âœ… Day 2 â€“ Apache Spark Fundamentals
- Spark architecture (driver, executors, DAG)
- DataFrames vs RDDs
- Lazy evaluation
- Notebook magic commands (`%sql`, `%python`, `%fs`)

(Future days will be added progressively.)

---

## ğŸ“ Repository Structure
```
Databricks_Challenge/
â”‚
â”œâ”€â”€ Key Notes/
â”‚ â”œâ”€â”€ Points to remember.md
â”œâ”€â”€ diagrams/
â”‚ â”œâ”€â”€ apache spark component.png
â”‚ â”œâ”€â”€ lazy evaluation.jpg
â”‚ â””â”€â”€ master worker paradigm.png
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ Databricks platform setup and overview.md
â”‚ â””â”€â”€ Apache Spark Fundamentals IN DATABRICKS.pdf
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ load-data_from_csv.py
â”‚ â””â”€â”€ Day 2 Basic operations.ipynb
â””â”€â”€ README.md
```

---

## ğŸ“˜ Learning Notes & Key Takeaways

Along with daily learning folders, I maintain a living document
to capture my personal understanding and mental models.

ğŸ‘‰ [Points to Remember â€“ Key Learnings](https://github.com/JayaraniArunachalam/Databricks_Challenge/blob/main/Key%20Notes/Points%20to%20remember.md)

---

## ğŸš€ Next Steps
- Build Bronze Delta tables
- Implement Silver transformations
- Create Gold analytical views
- Automate pipelines using Databricks Jobs

---

ğŸ“Œ *This repository is continuously updated as part of the learning journey.*

